{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Drving: Vehicle Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "VEHICLE_DIR = \"./train/vehicles/\"\n",
    "NON_VEHICLE_DIR = \"./train/non-vehicles/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(image, color_space='RGB', size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    color_space_dict = {\"HSV\": cv2.COLOR_RGB2HSV, \"HLS\": cv2.COLOR_RGB2HLS, \"YCrCb\" :cv2.COLOR_RGB2YCrCb}\n",
    "    \n",
    "    img = cv2.resize(image, size)\n",
    "    if color_space in color_space_dict.keys():\n",
    "        image = cv2.cvtColor(image, color_space_dict[color_space])\n",
    "    \n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = image.ravel() # Remove this line!\n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "def extract_features(image_path_list, color_channel, orientations, pix_per_cell, cell_per_block):\n",
    "    COLOR_CHANNEL_DICT = {\"HLS:S\": (cv2.COLOR_RGB2HLS, 2), \"YCrCb:Y\": (cv2.COLOR_RGB2YCrCb, 0), \"YCrCb:Cr\": (cv2.COLOR_RGB2YCrCb, 1)}\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for image_path in image_path_list:\n",
    "        image = read_image(image_path)\n",
    "    \n",
    "        if  color_channel in COLOR_CHANNEL_DICT.keys():\n",
    "            image = cv2.cvtColor(image, COLOR_CHANNEL_DICT[color_channel][0])[:,:,COLOR_CHANNEL_DICT[color_channel][1]]\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "        hog_features = get_hog_features(image, orientations, pix_per_cell, cell_per_block,  vis=False, feature_vec=True)\n",
    "        features.append(hog_features)\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicle_image_path_list = glob.glob(VEHICLE_DIR + \"./*/*.png\")\n",
    "non_vehicle_image_path_list = glob.glob(NON_VEHICLE_DIR + \"./*/*.png\")\n",
    "\n",
    "print(\"vehicle image count: \" + str(len(vehicle_image_path_list)) + \" non vehicle image count: \" + str(len(non_vehicle_image_path_list)))\n",
    "\n",
    "#example output\n",
    "for i in range(3):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    f.tight_layout()\n",
    "    vehicle_image = read_image(random.choice(vehicle_image_path_list))\n",
    "    non_vehicle_image = read_image(random.choice(non_vehicle_image_path_list))\n",
    "    ax1.imshow(vehicle_image)\n",
    "    ax1.set_title(\"vehicle \" + str(vehicle_image.shape))\n",
    "    ax2.imshow(non_vehicle_image)\n",
    "    ax2.set_title(\"non vehicle \" + str(non_vehicle_image.shape))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracted HOG Features from the Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLOR_CHANNEL_DICT = {\"HLS:S\": (cv2.COLOR_RGB2HLS, 2), \"YCrCb:Y\": (cv2.COLOR_RGB2YCrCb, 0), \"YCrCb:Cr\": (cv2.COLOR_RGB2YCrCb, 1)}\n",
    "\n",
    "orientations = [8, 9]\n",
    "pixels_per_cell = [6, 8]\n",
    "cells_per_block = [2, 3]\n",
    "\n",
    "parameter_combination_list = list(itertools.product(orientations, pixels_per_cell, cells_per_block))\n",
    "\n",
    "car_image_path = random.choice(vehicle_image_path_list)\n",
    "not_car_image_path = random.choice(non_vehicle_image_path_list)\n",
    "\n",
    "for color_channel in COLOR_CHANNEL_DICT.keys():\n",
    "    print(color_channel)\n",
    "    car_image = read_image(car_image_path)\n",
    "    not_car_image = read_image(not_car_image_path)\n",
    "    \n",
    "    color_space = COLOR_CHANNEL_DICT[color_channel][0]\n",
    "    color_channel_index = COLOR_CHANNEL_DICT[color_channel][1] \n",
    "    \n",
    "    car_image = cv2.cvtColor(car_image,color_space)[:,:,color_channel_index]\n",
    "    not_car_image = cv2.cvtColor(not_car_image, color_space)[:,:,color_channel_index]\n",
    "    \n",
    "    for parameter_combination in parameter_combination_list: \n",
    "        car_features, car_hog_image = get_hog_features(car_image, parameter_combination[0], parameter_combination[1], parameter_combination[2], \n",
    "                        vis=True, feature_vec=True)\n",
    "        \n",
    "        not_car_features, not_car_hog_image = get_hog_features(not_car_image, parameter_combination[0], parameter_combination[1], parameter_combination[2], \n",
    "                        vis=True, feature_vec=True)\n",
    "        \n",
    "        f, axes = plt.subplots(1, 4, figsize=(16, 8))\n",
    "        \n",
    "        axes = axes.ravel()\n",
    "        axes[0].imshow(car_image,cmap='gray')\n",
    "        axes[0].set_title(\"car in \" + color_channel)\n",
    "        \n",
    "        axes[1].imshow(car_hog_image,cmap='gray')\n",
    "        axes[1].set_title(\"car HOG \\n (orientations,pixels_per_cell, cells_per_block) = \\n\" + str(parameter_combination))\n",
    "        \n",
    "        axes[2].imshow(not_car_image, cmap='gray')\n",
    "        axes[2].set_title(\"not car in \" + color_channel)\n",
    "        \n",
    "        axes[3].imshow(not_car_hog_image,cmap='gray')\n",
    "        axes[3].set_title(\"not car HOG\\n (orientations,pixels_per_cell, cells_per_block) = \\n\" + str(parameter_combination))\n",
    "        \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classfier Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicle_features = extract_features(vehicle_image_path_list, \"YCrCb:Y\", orientations = 8, pix_per_cell = 8, cell_per_block = 2)\n",
    "non_vehicle_features = extract_features(non_vehicle_image_path_list, \"YCrCb:Y\", orientations = 8, pix_per_cell = 8, cell_per_block = 2)\n",
    "\n",
    "print(\"vehicle image count: \" + str(len(vehicle_features)) + \" , non vehicle image count: \" + str(len(non_vehicle_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_X = np.vstack((vehicle_features, non_vehicle_features)).astype(np.float64)\n",
    "scaled_X = StandardScaler().fit_transform(raw_X)\n",
    "print(\"X shape: \" + str(scaled_X.shape))\n",
    "y = np.hstack((np.ones(len(vehicle_features)), np.zeros(len(non_vehicle_features))))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "parameter_dict = {'kernel':('linear','rbf'), 'C':[0.1,1,10]}\n",
    "svr = SVC()\n",
    "clf = GridSearchCV(svr, parameter_dict)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Fit Parameter: \", clf.best_estimator_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy = {:.4f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
